{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjYvdj53hFYRx7srnd1AjA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManupatiEshwar/reniforecement/blob/main/Assignment6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AU5xdaNHKZc",
        "outputId": "27f66738-67b8-4308-e4b7-46d91f941c75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium==0.29.1 in /usr/local/lib/python3.12/dist-packages (0.29.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Step 5000, Avg Reward (last 10 episodes): 19.0\n",
            "Step 10000, Avg Reward (last 10 episodes): 19.0\n",
            "Step 15000, Avg Reward (last 10 episodes): 50.7\n",
            "Step 20000, Avg Reward (last 10 episodes): 64.1\n",
            "Training finished. Final average reward: 64.1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install gymnasium==0.29.1 torch torchvision\n",
        "\n",
        "\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gymnasium as gym\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "\n",
        "Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity, obs_shape, device):\n",
        "        self.capacity = capacity\n",
        "        self.device = device\n",
        "        self.pos = 0\n",
        "        self.full = False\n",
        "        self.obs_buf = np.zeros((capacity, *obs_shape), dtype=np.float32)\n",
        "        self.next_obs_buf = np.zeros((capacity, *obs_shape), dtype=np.float32)\n",
        "        self.acts = np.zeros((capacity,), dtype=np.int64)\n",
        "        self.rews = np.zeros((capacity,), dtype=np.float32)\n",
        "        self.dones = np.zeros((capacity,), dtype=np.bool_)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.obs_buf[self.pos] = state\n",
        "        self.acts[self.pos] = action\n",
        "        self.rews[self.pos] = reward\n",
        "        self.next_obs_buf[self.pos] = next_state\n",
        "        self.dones[self.pos] = done\n",
        "        self.pos = (self.pos + 1) % self.capacity\n",
        "        self.full = self.full or self.pos == 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.capacity if self.full else self.pos\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        idxs = np.random.randint(0, len(self), size=batch_size)\n",
        "        s = torch.from_numpy(self.obs_buf[idxs]).to(self.device)\n",
        "        ns = torch.from_numpy(self.next_obs_buf[idxs]).to(self.device)\n",
        "        a = torch.from_numpy(self.acts[idxs]).to(self.device)\n",
        "        r = torch.from_numpy(self.rews[idxs]).to(self.device)\n",
        "        d = torch.from_numpy(self.dones[idxs]).to(self.device)\n",
        "        return s, a, r, ns, d\n",
        "\n",
        "\n",
        "class MLPQ(nn.Module):\n",
        "    def __init__(self, obs_dim, n_actions):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 128), nn.ReLU(),\n",
        "            nn.Linear(128, n_actions)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def train_dqn(env_name=\"CartPole-v1\", total_steps=20_000, batch_size=64, gamma=0.99, lr=1e-3,\n",
        "              buffer_size=50_000, start_steps=1000, update_every=50, target_update=1000):\n",
        "\n",
        "    env = gym.make(env_name)\n",
        "    obs_shape = env.observation_space.shape\n",
        "    n_actions = env.action_space.n\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    policy_net = MLPQ(obs_dim=obs_shape[0], n_actions=n_actions).to(device)\n",
        "    target_net = MLPQ(obs_dim=obs_shape[0], n_actions=n_actions).to(device)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
        "    buffer = ReplayBuffer(buffer_size, obs_shape, device)\n",
        "\n",
        "    obs, _ = env.reset()\n",
        "    episode_reward = 0\n",
        "    rewards = []\n",
        "\n",
        "    for step in range(1, total_steps + 1):\n",
        "        # epsilon-greedy\n",
        "        epsilon = max(0.01, 1 - step / 20_000)\n",
        "        if random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                q_vals = policy_net(torch.tensor(obs, dtype=torch.float32, device=device))\n",
        "                action = q_vals.argmax().item()\n",
        "\n",
        "        next_obs, reward, done, truncated, _ = env.step(action)\n",
        "        buffer.push(obs, action, reward, next_obs, done or truncated)\n",
        "        obs = next_obs\n",
        "        episode_reward += reward\n",
        "\n",
        "        if done or truncated:\n",
        "            rewards.append(episode_reward)\n",
        "            obs, _ = env.reset()\n",
        "            episode_reward = 0\n",
        "\n",
        "        # train step\n",
        "        if step > start_steps and step % update_every == 0 and len(buffer) > batch_size:\n",
        "            s, a, r, ns, d = buffer.sample(batch_size)\n",
        "            q_values = policy_net(s).gather(1, a.unsqueeze(1)).squeeze()\n",
        "            with torch.no_grad():\n",
        "                max_next_q = target_net(ns).max(1)[0]\n",
        "                target = r + gamma * max_next_q * (1 - d.float())\n",
        "            loss = nn.MSELoss()(q_values, target)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # update target net\n",
        "        if step % target_update == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "        if step % 5000 == 0:\n",
        "            avg_r = np.mean(rewards[-10:]) if rewards else 0\n",
        "            print(f\"Step {step}, Avg Reward (last 10 episodes): {avg_r}\")\n",
        "\n",
        "    env.close()\n",
        "    return rewards\n",
        "\n",
        "\n",
        "rewards = train_dqn(\"CartPole-v1\", total_steps=20_000)\n",
        "\n",
        "print(\"Training finished. Final average reward:\", np.mean(rewards[-10:]))\n"
      ]
    }
  ]
}