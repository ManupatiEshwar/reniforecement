{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIABwexN4ORdhfeyPUG6YX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManupatiEshwar/reniforecement/blob/main/Lab9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hDmlj3jQXvB",
        "outputId": "cc216291-e8ca-4c53-e13f-0577527045d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps: 1024\tLoss: 60.891\tPolicy: 0.037\tValue: 121.721\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps: 2048\tLoss: 51.405\tPolicy: -0.050\tValue: 102.924\n",
            "Steps: 3072\tLoss: 57.448\tPolicy: -0.126\tValue: 115.161\n",
            "Steps: 4096\tLoss: 54.891\tPolicy: -0.028\tValue: 109.848\n",
            "Steps: 5120\tLoss: 52.243\tPolicy: -0.102\tValue: 104.699\n",
            "Steps: 6144\tLoss: 48.382\tPolicy: -0.079\tValue: 96.931\n",
            "Steps: 7168\tLoss: 43.891\tPolicy: 0.066\tValue: 87.662\n",
            "Steps: 8192\tLoss: 39.998\tPolicy: 0.120\tValue: 79.767\n",
            "Steps: 9216\tLoss: 35.917\tPolicy: 0.240\tValue: 71.366\n",
            "Steps: 10240\tLoss: 34.035\tPolicy: -0.148\tValue: 68.376\n",
            "Steps: 11264\tLoss: 30.192\tPolicy: -0.017\tValue: 60.428\n",
            "Steps: 12288\tLoss: 27.707\tPolicy: -0.147\tValue: 55.718\n",
            "Steps: 13312\tLoss: 24.407\tPolicy: 0.119\tValue: 48.586\n",
            "Steps: 14336\tLoss: 21.574\tPolicy: 0.275\tValue: 42.608\n",
            "Steps: 15360\tLoss: 19.254\tPolicy: 0.266\tValue: 37.987\n",
            "Steps: 16384\tLoss: 17.417\tPolicy: -0.029\tValue: 34.904\n",
            "Steps: 17408\tLoss: 15.290\tPolicy: 0.050\tValue: 30.492\n",
            "Steps: 18432\tLoss: 13.448\tPolicy: 0.222\tValue: 26.465\n",
            "Steps: 19456\tLoss: 11.671\tPolicy: -0.012\tValue: 23.380\n",
            "Steps: 20480\tLoss: 10.253\tPolicy: 0.209\tValue: 20.102\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "env_name = \"CartPole-v1\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim):\n",
        "        super().__init__()\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(obs_dim, 64), nn.Tanh(),\n",
        "            nn.Linear(64, 64), nn.Tanh()\n",
        "        )\n",
        "        self.policy = nn.Sequential(nn.Linear(64, act_dim), nn.Softmax(dim=-1))\n",
        "        self.value = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.shared(x)\n",
        "        return self.policy(h), self.value(h)\n",
        "\n",
        "\n",
        "def collect_trajectories(env, net, steps, gamma, lam):\n",
        "    obs, _ = env.reset()\n",
        "    obs_buf, act_buf, rew_buf, val_buf, logp_buf = [], [], [], [], []\n",
        "    ep_rews, ep_len = [], []\n",
        "\n",
        "    for _ in range(steps):\n",
        "        obs_t = torch.as_tensor(obs, dtype=torch.float32).to(device)\n",
        "        pi, v = net(obs_t)\n",
        "        dist = torch.distributions.Categorical(pi)\n",
        "        a = dist.sample().cpu().numpy()\n",
        "        logp = dist.log_prob(torch.as_tensor(a)).cpu().item()\n",
        "\n",
        "        obs_buf.append(obs.copy())\n",
        "        act_buf.append(a)\n",
        "        val_buf.append(v.cpu().item())\n",
        "        logp_buf.append(logp)\n",
        "\n",
        "        next_obs, r, terminated, truncated, _ = env.step(int(a))\n",
        "        done = terminated or truncated\n",
        "        rew_buf.append(r)\n",
        "        ep_rews.append(r)\n",
        "\n",
        "        obs = next_obs\n",
        "        if done:\n",
        "            obs, _ = env.reset()\n",
        "            ep_len.append(len(ep_rews))\n",
        "            ep_rews = []\n",
        "\n",
        "    obs_buf, act_buf = np.array(obs_buf), np.array(act_buf)\n",
        "    rew_buf, val_buf, logp_buf = np.array(rew_buf), np.array(val_buf), np.array(logp_buf)\n",
        "\n",
        "    last_val = net(torch.as_tensor(obs, dtype=torch.float32).to(device))[1].cpu().item()\n",
        "    adv_buf = np.zeros_like(rew_buf)\n",
        "    lastgaelam = 0\n",
        "\n",
        "    for t in reversed(range(len(rew_buf))):\n",
        "        if t == len(rew_buf) - 1:\n",
        "            nextnonterminal = 1.0\n",
        "            nextvalues = last_val\n",
        "        else:\n",
        "            nextnonterminal = 1.0\n",
        "            nextvalues = val_buf[t + 1]\n",
        "        delta = rew_buf[t] + gamma * nextvalues * nextnonterminal - val_buf[t]\n",
        "        lastgaelam = delta + gamma * lam * nextnonterminal * lastgaelam\n",
        "        adv_buf[t] = lastgaelam\n",
        "\n",
        "    ret_buf = adv_buf + val_buf\n",
        "    return obs_buf, act_buf, logp_buf, adv_buf, ret_buf\n",
        "\n",
        "\n",
        "def ppo_train(env_name=\"CartPole-v1\", total_steps=20000, batch_steps=1024, epochs=10, minibatch_size=64,\n",
        "              gamma=0.99, lam=0.95, clip=0.2, pi_lr=3e-4):\n",
        "    env = gym.make(env_name)\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    act_dim = env.action_space.n\n",
        "    net = ActorCritic(obs_dim, act_dim).to(device)\n",
        "    optimizer = optim.Adam(net.parameters(), lr=pi_lr)\n",
        "    steps = 0\n",
        "\n",
        "    while steps < total_steps:\n",
        "        obs_buf, act_buf, logp_buf, adv_buf, ret_buf = collect_trajectories(env, net, batch_steps, gamma, lam)\n",
        "        steps += batch_steps\n",
        "        adv_buf = (adv_buf - adv_buf.mean()) / (adv_buf.std() + 1e-8)\n",
        "        inds = np.arange(batch_steps)\n",
        "\n",
        "        for _ in range(epochs):\n",
        "            np.random.shuffle(inds)\n",
        "            for start in range(0, batch_steps, minibatch_size):\n",
        "                mb = inds[start:start + minibatch_size]\n",
        "                obs_mb = torch.as_tensor(obs_buf[mb], dtype=torch.float32).to(device)\n",
        "                act_mb = torch.as_tensor(act_buf[mb], dtype=torch.int64).to(device)\n",
        "                old_logp_mb = torch.as_tensor(logp_buf[mb], dtype=torch.float32).to(device)\n",
        "                adv_mb = torch.as_tensor(adv_buf[mb], dtype=torch.float32).to(device)\n",
        "                ret_mb = torch.as_tensor(ret_buf[mb], dtype=torch.float32).to(device)\n",
        "\n",
        "                pi, v = net(obs_mb)\n",
        "                dist = torch.distributions.Categorical(pi)\n",
        "                logp = dist.log_prob(act_mb)\n",
        "                ratio = torch.exp(logp - old_logp_mb)\n",
        "                surr1 = ratio * adv_mb\n",
        "                surr2 = torch.clamp(ratio, 1 - clip, 1 + clip) * adv_mb\n",
        "                policy_loss = -torch.min(surr1, surr2).mean()\n",
        "                value_loss = ((v.squeeze(-1) - ret_mb) ** 2).mean()\n",
        "                entropy = dist.entropy().mean()\n",
        "                loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        print(f\"Steps: {steps}\\tLoss: {loss.item():.3f}\\tPolicy: {policy_loss.item():.3f}\\tValue: {value_loss.item():.3f}\")\n",
        "    env.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ppo_train(env_name)\n"
      ]
    }
  ]
}