{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzAyLrc1H0m5d52UiPgVRd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManupatiEshwar/reniforecement/blob/main/Lab7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrxFP6zcm2-n",
        "outputId": "1c1dc2bc-21a5-4595-848d-e42f5a8bd7b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 10\tEpisode reward: 33.00\tAvg100: 21.30\n",
            "Episode 20\tEpisode reward: 36.00\tAvg100: 26.55\n",
            "Episode 30\tEpisode reward: 12.00\tAvg100: 27.87\n",
            "Episode 40\tEpisode reward: 88.00\tAvg100: 33.00\n",
            "Episode 50\tEpisode reward: 163.00\tAvg100: 41.28\n",
            "Episode 60\tEpisode reward: 96.00\tAvg100: 43.47\n",
            "Episode 70\tEpisode reward: 196.00\tAvg100: 51.90\n",
            "Episode 80\tEpisode reward: 39.00\tAvg100: 56.66\n",
            "Episode 90\tEpisode reward: 87.00\tAvg100: 59.92\n",
            "Episode 100\tEpisode reward: 203.00\tAvg100: 72.72\n",
            "Episode 110\tEpisode reward: 133.00\tAvg100: 93.01\n",
            "Episode 120\tEpisode reward: 500.00\tAvg100: 113.04\n",
            "Episode 130\tEpisode reward: 110.00\tAvg100: 121.27\n",
            "Episode 140\tEpisode reward: 64.00\tAvg100: 125.45\n",
            "Episode 150\tEpisode reward: 47.00\tAvg100: 124.41\n",
            "Episode 160\tEpisode reward: 227.00\tAvg100: 134.01\n",
            "Episode 170\tEpisode reward: 30.00\tAvg100: 139.85\n",
            "Episode 180\tEpisode reward: 50.00\tAvg100: 136.43\n",
            "Episode 190\tEpisode reward: 73.00\tAvg100: 133.04\n",
            "Episode 200\tEpisode reward: 198.00\tAvg100: 126.60\n",
            "Episode 210\tEpisode reward: 131.00\tAvg100: 130.64\n",
            "Episode 220\tEpisode reward: 166.00\tAvg100: 122.60\n",
            "Episode 230\tEpisode reward: 129.00\tAvg100: 126.49\n",
            "Episode 240\tEpisode reward: 213.00\tAvg100: 133.02\n",
            "Episode 250\tEpisode reward: 295.00\tAvg100: 153.49\n",
            "Episode 260\tEpisode reward: 500.00\tAvg100: 186.68\n",
            "Episode 270\tEpisode reward: 500.00\tAvg100: 217.34\n",
            "Episode 280\tEpisode reward: 500.00\tAvg100: 259.30\n",
            "Episode 290\tEpisode reward: 500.00\tAvg100: 301.68\n",
            "Episode 300\tEpisode reward: 500.00\tAvg100: 339.33\n",
            "Episode 310\tEpisode reward: 500.00\tAvg100: 361.87\n",
            "Episode 320\tEpisode reward: 500.00\tAvg100: 394.72\n",
            "Episode 330\tEpisode reward: 500.00\tAvg100: 422.97\n",
            "Episode 340\tEpisode reward: 500.00\tAvg100: 457.10\n",
            "Episode 350\tEpisode reward: 413.00\tAvg100: 474.23\n",
            "Episode 360\tEpisode reward: 500.00\tAvg100: 474.57\n",
            "Episode 370\tEpisode reward: 500.00\tAvg100: 470.91\n",
            "Episode 380\tEpisode reward: 144.00\tAvg100: 453.64\n",
            "Episode 390\tEpisode reward: 119.00\tAvg100: 418.06\n",
            "Episode 400\tEpisode reward: 99.00\tAvg100: 378.67\n",
            "Episode 410\tEpisode reward: 139.00\tAvg100: 342.93\n",
            "Episode 420\tEpisode reward: 136.00\tAvg100: 309.44\n",
            "Episode 430\tEpisode reward: 159.00\tAvg100: 279.75\n",
            "Episode 440\tEpisode reward: 186.00\tAvg100: 246.07\n",
            "Episode 450\tEpisode reward: 225.00\tAvg100: 222.57\n",
            "Episode 460\tEpisode reward: 500.00\tAvg100: 215.86\n",
            "Episode 470\tEpisode reward: 500.00\tAvg100: 219.65\n",
            "Episode 480\tEpisode reward: 233.00\tAvg100: 233.33\n",
            "Episode 490\tEpisode reward: 412.00\tAvg100: 255.29\n",
            "Episode 500\tEpisode reward: 317.00\tAvg100: 276.65\n",
            "Episode 510\tEpisode reward: 500.00\tAvg100: 300.91\n",
            "Episode 520\tEpisode reward: 366.00\tAvg100: 334.22\n",
            "Episode 530\tEpisode reward: 337.00\tAvg100: 362.19\n",
            "Episode 540\tEpisode reward: 207.00\tAvg100: 373.80\n",
            "Episode 550\tEpisode reward: 351.00\tAvg100: 381.59\n",
            "Episode 560\tEpisode reward: 500.00\tAvg100: 386.14\n",
            "Episode 570\tEpisode reward: 299.00\tAvg100: 379.32\n",
            "Episode 580\tEpisode reward: 500.00\tAvg100: 370.14\n",
            "Episode 590\tEpisode reward: 500.00\tAvg100: 385.75\n",
            "Episode 600\tEpisode reward: 500.00\tAvg100: 397.78\n",
            "Episode 610\tEpisode reward: 500.00\tAvg100: 407.74\n",
            "Episode 620\tEpisode reward: 103.00\tAvg100: 403.47\n",
            "Episode 630\tEpisode reward: 231.00\tAvg100: 395.42\n",
            "Episode 640\tEpisode reward: 98.00\tAvg100: 384.65\n",
            "Episode 650\tEpisode reward: 36.00\tAvg100: 367.29\n",
            "Episode 660\tEpisode reward: 117.00\tAvg100: 331.60\n",
            "Episode 670\tEpisode reward: 112.00\tAvg100: 302.17\n",
            "Episode 680\tEpisode reward: 32.00\tAvg100: 275.57\n",
            "Episode 690\tEpisode reward: 99.00\tAvg100: 236.29\n",
            "Episode 700\tEpisode reward: 109.00\tAvg100: 200.98\n",
            "Episode 710\tEpisode reward: 135.00\tAvg100: 163.57\n",
            "Episode 720\tEpisode reward: 120.00\tAvg100: 131.90\n",
            "Episode 730\tEpisode reward: 110.00\tAvg100: 108.92\n",
            "Episode 740\tEpisode reward: 116.00\tAvg100: 103.62\n",
            "Episode 750\tEpisode reward: 127.00\tAvg100: 106.41\n",
            "Episode 760\tEpisode reward: 40.00\tAvg100: 108.48\n",
            "Episode 770\tEpisode reward: 125.00\tAvg100: 111.41\n",
            "Episode 780\tEpisode reward: 128.00\tAvg100: 116.49\n",
            "Episode 790\tEpisode reward: 153.00\tAvg100: 119.82\n",
            "Episode 800\tEpisode reward: 152.00\tAvg100: 126.49\n",
            "Episode 810\tEpisode reward: 214.00\tAvg100: 135.45\n",
            "Episode 820\tEpisode reward: 290.00\tAvg100: 148.75\n",
            "Episode 830\tEpisode reward: 473.00\tAvg100: 170.40\n",
            "Episode 840\tEpisode reward: 493.00\tAvg100: 203.54\n",
            "Episode 850\tEpisode reward: 457.00\tAvg100: 238.14\n",
            "Episode 860\tEpisode reward: 500.00\tAvg100: 264.43\n",
            "Episode 870\tEpisode reward: 500.00\tAvg100: 300.87\n",
            "Episode 880\tEpisode reward: 500.00\tAvg100: 337.62\n",
            "Episode 890\tEpisode reward: 500.00\tAvg100: 373.99\n",
            "Episode 900\tEpisode reward: 500.00\tAvg100: 408.63\n",
            "Episode 910\tEpisode reward: 500.00\tAvg100: 439.60\n",
            "Episode 920\tEpisode reward: 500.00\tAvg100: 464.40\n",
            "Episode 930\tEpisode reward: 500.00\tAvg100: 481.96\n",
            "Solved! Avg reward 481.96 over 100 episodes. Stopping.\n",
            "Evaluation (no render):\n",
            "Eval episode 1: reward = 500.0\n",
            "Eval episode 2: reward = 500.0\n",
            "Eval episode 3: reward = 500.0\n",
            "Eval episode 4: reward = 500.0\n",
            "Eval episode 5: reward = 500.0\n",
            "Average eval reward: 500.00\n"
          ]
        }
      ],
      "source": [
        "# REINFORCE on CartPole-v1 (PyTorch + gymnasium)\n",
        "# Save as reinforce_cartpole.py or run in a notebook cell.\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gymnasium as gym\n",
        "from collections import deque\n",
        "\n",
        "# ---------- Hyperparameters ----------\n",
        "ENV_NAME = \"CartPole-v1\"\n",
        "GAMMA = 0.99\n",
        "LR = 1e-3\n",
        "HIDDEN = 128\n",
        "BATCH_SIZE = 1          # for REINFORCE with full-episode updates, batch=1 (one episode)\n",
        "MAX_EPISODES = 2000\n",
        "MAX_STEPS_PER_EP = 1000\n",
        "REWARD_TO_GO = True     # True: use reward-to-go; False: use full-episode return\n",
        "ENTROPY_COEF = 0.0      # small entropy bonus can help exploration\n",
        "SOLVED_SCORE = 475.0    # CartPole-v1 considered solved around 475-500 over 500 episode avg\n",
        "LOG_INTERVAL = 10\n",
        "# -------------------------------------\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Simple policy network -> returns action probabilities\n",
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, obs_dim, action_dim, hidden=HIDDEN):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, action_dim),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        logits = self.net(x)\n",
        "        return torch.softmax(logits, dim=-1), logits  # return probs and logits (for entropy)\n",
        "\n",
        "def discount_rewards(rewards, gamma=GAMMA):\n",
        "    \"\"\"Compute discounted returns for an episode (full return for each time or reward-to-go).\"\"\"\n",
        "    if REWARD_TO_GO:\n",
        "        # reward-to-go: for each t, G_t = sum_{t'=t..T-1} gamma^{t'-t} r_{t'}\n",
        "        n = len(rewards)\n",
        "        rtg = np.zeros(n, dtype=np.float32)\n",
        "        running = 0.0\n",
        "        for i in reversed(range(n)):\n",
        "            running = rewards[i] + gamma * running\n",
        "            rtg[i] = running\n",
        "        return rtg\n",
        "    else:\n",
        "        # full-episode return: same return for all steps\n",
        "        total = 0.0\n",
        "        for r in rewards:\n",
        "            total = total * gamma + r if False else total + r  # not used; simpler below\n",
        "        # simpler: standard undiscounted full return but let's use discounted full return\n",
        "        # compute discounted sum from start:\n",
        "        total = 0.0\n",
        "        pow = 1.0\n",
        "        for r in rewards:\n",
        "            total += pow * r\n",
        "            pow *= gamma\n",
        "        return np.array([total] * len(rewards), dtype=np.float32)\n",
        "\n",
        "def train():\n",
        "    env = gym.make(ENV_NAME, render_mode=None)\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    policy = PolicyNet(obs_dim, action_dim).to(device)\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=LR)\n",
        "\n",
        "    running_rewards = deque(maxlen=100)\n",
        "    best_avg = -float('inf')\n",
        "\n",
        "    for episode in range(1, MAX_EPISODES + 1):\n",
        "        obs, _ = env.reset()\n",
        "        obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
        "\n",
        "        log_probs = []\n",
        "        logits_list = []\n",
        "        rewards = []\n",
        "        episode_reward = 0.0\n",
        "\n",
        "        # collect one episode\n",
        "        for step in range(MAX_STEPS_PER_EP):\n",
        "            probs, logits = policy(obs.unsqueeze(0))   # shape (1, action_dim)\n",
        "            probs = probs.squeeze(0)\n",
        "            logits = logits.squeeze(0)\n",
        "\n",
        "            m = torch.distributions.Categorical(probs)\n",
        "            action = m.sample()\n",
        "            log_prob = m.log_prob(action)\n",
        "            entropy = m.entropy()\n",
        "\n",
        "            next_obs, reward, terminated, truncated, info = env.step(int(action.item()))\n",
        "            done = terminated or truncated\n",
        "\n",
        "            log_probs.append(log_prob)\n",
        "            logits_list.append(logits)   # for optional entropy calc\n",
        "            rewards.append(float(reward))\n",
        "            episode_reward += float(reward)\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            obs = torch.tensor(next_obs, dtype=torch.float32, device=device)\n",
        "\n",
        "        running_rewards.append(episode_reward)\n",
        "\n",
        "        # compute returns (discounted)\n",
        "        returns = discount_rewards(rewards, gamma=GAMMA)\n",
        "        # normalize returns for stability (common trick)\n",
        "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
        "        returns = torch.tensor(returns, dtype=torch.float32, device=device)\n",
        "\n",
        "        # compute policy loss: -sum_t log_prob_t * return_t\n",
        "        policy_loss = 0.0\n",
        "        entropy_term = 0.0\n",
        "        for lp, logit, G in zip(log_probs, logits_list, returns):\n",
        "            policy_loss += -lp * G\n",
        "            if ENTROPY_COEF > 0:\n",
        "                # entropy from logits (stable)\n",
        "                probs = torch.softmax(logit, dim=-1)\n",
        "                ent = -(probs * torch.log(probs + 1e-8)).sum()\n",
        "                entropy_term += ent\n",
        "\n",
        "        if ENTROPY_COEF > 0:\n",
        "            policy_loss = policy_loss - ENTROPY_COEF * entropy_term\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # logging\n",
        "        if episode % LOG_INTERVAL == 0:\n",
        "            avg100 = np.mean(running_rewards) if running_rewards else 0.0\n",
        "            print(f\"Episode {episode}\\tEpisode reward: {episode_reward:.2f}\\tAvg100: {avg100:.2f}\")\n",
        "\n",
        "            if avg100 > best_avg:\n",
        "                best_avg = avg100\n",
        "                # optionally save\n",
        "                torch.save(policy.state_dict(), \"reinforce_policy.pth\")\n",
        "\n",
        "            if avg100 >= SOLVED_SCORE and len(running_rewards) >= 100:\n",
        "                print(f\"Solved! Avg reward {avg100:.2f} over 100 episodes. Stopping.\")\n",
        "                break\n",
        "\n",
        "    env.close()\n",
        "    return policy\n",
        "\n",
        "def evaluate(policy, episodes=10, render=False):\n",
        "    env = gym.make(ENV_NAME, render_mode=\"human\" if render else None)\n",
        "    total = 0.0\n",
        "    for ep in range(episodes):\n",
        "        obs, _ = env.reset()\n",
        "        ep_r = 0.0\n",
        "        for _ in range(1000):\n",
        "            obs_v = torch.tensor(obs, dtype=torch.float32).to(device)\n",
        "            probs, _ = policy(obs_v.unsqueeze(0))\n",
        "            action = torch.argmax(probs, dim=-1).item()\n",
        "            obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            ep_r += reward\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "        total += ep_r\n",
        "        print(f\"Eval episode {ep+1}: reward = {ep_r}\")\n",
        "    env.close()\n",
        "    print(f\"Average eval reward: {total / episodes:.2f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    trained_policy = train()\n",
        "    print(\"Evaluation (no render):\")\n",
        "    evaluate(trained_policy, episodes=5, render=False)\n"
      ]
    }
  ]
}